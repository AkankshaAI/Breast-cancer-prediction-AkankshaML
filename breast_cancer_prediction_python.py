# -*- coding: utf-8 -*-
"""Breast_Cancer_Prediction_Python.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bqiRlbWvBUO8MhoZV-mZ0lhaNvuGxB6Y

# Breast Cancer Prediction 
Project By 
1.Akanksha Rani-10601182021

# Importing  python libraries
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns 
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score ,precision_score,f1_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.model_selection import KFold
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
import time

"""# Importing data from drive """

import pandas as pd 
from google.colab import drive
print("importing csv file from drive") 
drive.mount('/content/drive')
path="/content/drive/MyDrive/Breast_cancer_data.csv"
df=pd.read_csv(path)
ch=input("want to display the dataframe?( Y/N):")
if ch=="Y" or ch=="y":
  print(df.head(10))
else:
  print("OK continuing the programme.") 
print("-*-"*20)

"""# Data Analysis"""

print(df.shape)
df.describe()
df.isnull().sum()
print("counting  non-null values..")
print("total non-null values in columns:-",df.count(1))
cols = list(df.columns)
print(cols)
sns.countplot(x='diagnosis', data=df, palette='RdBu_r')
plt.title('Displot for Diagnosis ')
plt.show()

"""# Data Visualization and Pre Processing """

#heatmap for all the parameters of diagnosis
corr = df.corr(method = 'spearman')  
plt.figure(figsize=(15,8))
sns.heatmap(corr, annot=True, fmt='.2f', cmap='Pastel1')
plt.title('Spearman Correlation Heatmap')
plt.show()

figure = plt.figure(figsize=(10,10))   # plotting for all the parameters 
sns.pairplot(df, hue='diagnosis', palette='Wistia')
plt.show()

fig, axes = plt.subplots(5, 3, figsize=(20,25))
for i, col in zip(range(5), cols):
    sns.stripplot(ax=axes[i][0], x='diagnosis', y=col, data=df, palette='autumn_r', jitter=True)
    axes[i][0].set_title(f'{col} Stripplot')
    sns.histplot(ax=axes[i][1], x=col, data=df, kde=True, bins=10, palette='Pastel2', hue='diagnosis', multiple='dodge')
    axes[i][1].set_title(f'{col} Displot')
    sns.boxplot(ax=axes[i][2], x='diagnosis', y=col, data=df, palette='Dark2', hue='diagnosis')
    axes[i][2].set_title(f'{col} Boxplot')

def outlier_limits(df, col_name, q1 = 0.25, q3 = 0.75):
    quartile1 = df[col_name].quantile(q1)
    quartile3 = df[col_name].quantile(q3)
    interquantile_range = quartile3 - quartile1
    up_limit = quartile3 + 1.5 * interquantile_range
    low_limit = quartile1 - 1.5 * interquantile_range
    return low_limit, up_limit

def replace_with_limits(df, variable, q1 = 0.25, q3 = 0.75):
    low_limit, up_limit = outlier_limits(df, variable, q1 = q1, q3 = q3)
    df.loc[(df[variable] < low_limit), variable] = low_limit
    df.loc[(df[variable] > up_limit), variable] = up_limit
    
for variable in cols:
    replace_with_limits(df, variable)

# showing box plot for all the parameters 
fig, axes = plt.subplots(1, 5, figsize=(25,6))
for i, col in zip(range(5), cols):
    sns.boxplot(ax=axes[i], x='diagnosis', y=col, data=df, palette='gnuplot', hue='diagnosis')
    axes[i].set_title(f'{col} Boxplot')

"""**PREPROCESSING**"""

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=cols)
df_scaled.head(10)

X= df_scaled.iloc[:,3:4]
y = df_scaled['diagnosis']
X.head(10)

"""### Missing Values"""

df.isnull().sum()

# Splitting the attributes into independent and dependent attributes
X = df.iloc[:, :-1].values # attributes to determine dependent variable / Class
Y = df.iloc[:, -1].values # dependent variable / Class

#training and testing the data set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)

X

Y

scoring = {
    'accuracy': 'accuracy',
    'precision': 'precision',
    'f1':'f1'}

def fit(clf, params, cv=10, X_train=X_train, y_train=y_train):
    grid = GridSearchCV(clf, params, cv=KFold(n_splits=cv), n_jobs=1, verbose=1, return_train_score=True, scoring=scoring, refit='f1') 
    grid.fit(X_train, y_train)
    return grid

def make_predictions(model, X_test=X_test):
    return model.predict(X_test)

def best_scores(model):
  best_mean_f1 = max(list(model.cv_results_['mean_test_f1']))
  mean_f1_index = list(model.cv_results_['mean_test_f1']).index(best_mean_f1)
  print(f'The best parameters are: {model.best_params_}')
  print('Mean Test Cross Validation Scores for different metrics: (corresponding to best mean f1)')
  print('The best score that we get is (Accuracy): ' + str(model.cv_results_['mean_test_accuracy'][mean_f1_index]))
  print('The best score that we get is (Precision): ' + str(model.cv_results_['mean_test_precision'][mean_f1_index]))
  print(f'The best score that we get is (F1 Score): {best_mean_f1}')
  return None
def plot_confusion_matrix(y_pred):
    print('00: True Negatives\n01: False Positives\n10: False Negatives\n11: True Positives\n')
    conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)
    fig, ax = plt.subplots(figsize=(6, 5))
    ax.matshow(conf_matrix, cmap='GnBu', alpha=0.75)
    for i in range(conf_matrix.shape[0]):
        for j in range(conf_matrix.shape[1]):
            ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='large') 
    plt.xlabel('Predictions', fontsize=20)
    plt.ylabel('Actuals', fontsize=20)
    plt.title('Confusion Matrix', fontsize=20)
    plt.show()
    return None
def check_scores(y_pred):
    print('Precision: %.3f' % precision_score(y_test, y_pred))
    print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))
    print('F1 Score: %.3f' % f1_score(y_test, y_pred))
    return None

import warnings
warnings.filterwarnings('always')

"""**LOGISTIC REGRESSION MODEL**"""

lr_params = {'C':[0.001,.009,0.01,.09,1,5,10,25], 'penalty':['l1', 'l2']} #lasso and ridge regression
lr_clf = LogisticRegression(solver='saga', max_iter=5000)
lr_model = fit(lr_clf, lr_params)

best_scores(lr_model)

lr_y_pred = make_predictions(lr_model)
check_scores(lr_y_pred)

"""**PLOTTING OF CONFUSION MATRIX FOR LINEAR REGRESSION MODEL**"""

plot_confusion_matrix(lr_y_pred)

""" **LINEAR DISCRIMINANT ANALYSIS (LDA)**







"""

lda_params = {'solver': ['svd', 'eigen']}
lda_clf =LDA()
lda_model = fit(lda_clf, lda_params)

best_scores(lda_model)

lda_y_pred = make_predictions(lda_model)
check_scores(lda_y_pred)

"""**Plotting of confusion matrix for LDA**"""

plot_confusion_matrix(lda_y_pred)

"""**SUPPORT VECTOR MACHINE (SVM)**"""

svm_params = {'C':[1,10,100,1000], 'gamma':[1,0.1,0.001,0.0001], 'kernel':['linear','rbf']}
svm_clf = SVC()
svm_model = fit(svm_clf, svm_params)

best_scores(svm_model)

svm_y_pred = make_predictions(svm_model)
check_scores(svm_y_pred)

"""**Plotting Confusion Matrix For SVM**"""

plot_confusion_matrix(svm_y_pred)

"""**K- NEAREST NEIGHBOURS (KNNs)**"""

knns_params = {'n_neighbors': list(range(1, 31)), 'weights': ['uniform', 'distance'], 
               'metric': ['euclidean', 'manhattan']}
knns_clf = KNeighborsClassifier()
knns_model = fit(knns_clf, knns_params)

best_scores(knns_model)

knns_y_pred = make_predictions(knns_model)
check_scores(knns_y_pred)

plot_confusion_matrix(knns_y_pred)

"""#CONCLUSION

*Four classification models Logical Regression,LDA, SVM and KNNs were used to clasify the data.
*   Hypertuning was done to get the best result
### * Variables were scaled for Logical Regression,KNN, SVM and LDA models.  
*  Each model provides high accuracy with Linear Discriminative Analysis being the lowest with 86% and all the other model provide an accuracy of 88%.

# SELECTION OF KNNs MODEL

* **bold text**  K-nearest neighbors is a non-parametric method used for classification and regression. It is one of the most easy ML technique used. It is a lazy learning model, with local approximation.

* Basic Theory :
The basic logic behind KNN is to explore your neighborhood, assume the test datapoint to be similar to them and derive the output. In KNN, we look for k neighbors and come up with the prediction.


*   Advantages :
Easy and simple machine learning model.
Few hyperparameters to tune.
*    KNN is better than SVM. SVM outperforms KNN when there are large features and lesser training data.


*  KNN is a completely non-parametric approach: no assumptions are made about the shape of the decision boundary.
Therefore, we can expect KNN to dominate LDA and logistic regression when the decision boundary is highly non-linear.
"""